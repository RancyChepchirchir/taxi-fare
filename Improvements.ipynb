{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Set random seed \n",
    "RSEED = 100\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "palette = sns.color_palette('Paired', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('input/train.csv', nrows = 5_000_000, \n",
    "                   parse_dates = ['pickup_datetime']).drop(columns = 'key')\n",
    "\n",
    "# Remove na\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['fare_amount'].between(left = 0, right = 100)]\n",
    "data = data.loc[data['passenger_count'] < 10]\n",
    "\n",
    "# Remove latitude and longtiude outliers\n",
    "data = data.loc[data['pickup_latitude'].between(40, 42)]\n",
    "data = data.loc[data['pickup_longitude'].between(-75, -72)]\n",
    "data = data.loc[data['dropoff_latitude'].between(40, 42)]\n",
    "data = data.loc[data['dropoff_longitude'].between(-75, -72)]\n",
    "\n",
    "# Bin the fare and convert to string\n",
    "data['fare-bin'] = pd.cut(data['fare_amount'], bins = list(range(0, 50, 5))).astype(str)\n",
    "\n",
    "# Uppermost bin\n",
    "data.loc[data['fare-bin'] == 'nan', 'fare-bin'] = '[45+]'\n",
    "\n",
    "# Adjust bin so the sorting is correct\n",
    "data.loc[data['fare-bin'] == '(5, 10]', 'fare-bin'] = '(05, 10]'\n",
    "\n",
    "# Bar plot of value counts\n",
    "data['fare-bin'].value_counts().sort_index().plot.bar(color = 'b', edgecolor = 'k');\n",
    "plt.title('Fare Binned');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute difference in latitude and longitude\n",
    "data['abs_lat_diff'] = (data['dropoff_latitude'] - data['pickup_latitude']).abs()\n",
    "data['abs_lon_diff'] = (data['dropoff_longitude'] - data['pickup_longitude']).abs()\n",
    "\n",
    "def minkowski_distance(x1, x2, y1, y2, p):\n",
    "    return ((abs(x2 - x1) ** p) + (abs(y2 - y1)) ** p) ** (1 / p)\n",
    "\n",
    "data['manhattan'] = minkowski_distance(data['pickup_longitude'], data['dropoff_longitude'],\n",
    "                                       data['pickup_latitude'], data['dropoff_latitude'], 1)\n",
    "\n",
    "data['euclidean'] = minkowski_distance(data['pickup_longitude'], data['dropoff_longitude'],\n",
    "                                       data['pickup_latitude'], data['dropoff_latitude'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('input/test.csv', parse_dates = ['pickup_datetime'])\n",
    "\n",
    "test['abs_lat_diff'] = (test['dropoff_latitude'] - test['pickup_latitude']).abs()\n",
    "test['abs_lon_diff'] = (test['dropoff_longitude'] - test['pickup_longitude']).abs()\n",
    "\n",
    "test['manhattan'] = minkowski_distance(test['pickup_longitude'], test['dropoff_longitude'],\n",
    "                                       test['pickup_latitude'], test['dropoff_latitude'], 1)\n",
    "\n",
    "test['euclidean'] = minkowski_distance(test['pickup_longitude'], test['dropoff_longitude'],\n",
    "                                       test['pickup_latitude'], test['dropoff_latitude'], 2)\n",
    "\n",
    "# Save the id for submission\n",
    "test_id = list(test.pop('key'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radius of hte earth in kilometers\n",
    "R = 6378\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.    \n",
    "    \n",
    "    source: https://stackoverflow.com/a/29546836\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # Find the differences\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    # Apply the formula \n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    # Calculate the angle (in radians)\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    # Convert to kilometers\n",
    "    km = R * c\n",
    "    \n",
    "    return km\n",
    "\n",
    "data['haversine'] =  haversine_np(data['pickup_longitude'], data['pickup_latitude'],\n",
    "                         data['dropoff_longitude'], data['dropoff_latitude']) \n",
    "\n",
    "test['haversine'] = haversine_np(test['pickup_longitude'], test['pickup_latitude'],\n",
    "                         test['dropoff_longitude'], test['dropoff_latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = RuntimeWarning)\n",
    "\n",
    "def metrics(train_pred, valid_pred, y_train, y_valid):\n",
    "    \"\"\"Calculate metrics:\n",
    "       Root mean squared error and mean absolute percentage error\"\"\"\n",
    "    \n",
    "    # Root mean squared error\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_pred))\n",
    "    \n",
    "    # Calculate absolute percentage error\n",
    "    train_ape = abs((y_train - train_pred) / y_train)\n",
    "    valid_ape = abs((y_valid - valid_pred) / y_valid)\n",
    "    \n",
    "    # Account for y values of 0\n",
    "    train_ape[train_ape == np.inf] = 0\n",
    "    train_ape[train_ape == -np.inf] = 0\n",
    "    valid_ape[valid_ape == np.inf] = 0\n",
    "    valid_ape[valid_ape == -np.inf] = 0\n",
    "    \n",
    "    train_mape = 100 * np.mean(train_ape)\n",
    "    valid_mape = 100 * np.mean(valid_ape)\n",
    "    \n",
    "    return train_rmse, valid_rmse, train_mape, valid_mape\n",
    "\n",
    "def evaluate(model, features, X_train, X_valid, y_train, y_valid):\n",
    "    \"\"\"Mean absolute percentage error\"\"\"\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = model.predict(X_train[features])\n",
    "    valid_pred = model.predict(X_valid[features])\n",
    "    \n",
    "    # Get metrics\n",
    "    train_rmse, valid_rmse, train_mape, valid_mape = metrics(train_pred, valid_pred,\n",
    "                                                             y_train, y_valid)\n",
    "    \n",
    "    print(f'Training:   rmse = {round(train_rmse, 2)} \\t mape = {round(train_mape, 2)}')\n",
    "    print(f'Validation: rmse = {round(valid_rmse, 2)} \\t mape = {round(valid_mape, 2)}')\n",
    "    \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def model_rf(X_train, X_valid, y_train, y_valid, test, features,\n",
    "             model = RandomForestRegressor(n_estimators = 25, max_depth = 25,\n",
    "                                           n_jobs = -1)):\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train[features], y_train)\n",
    "    \n",
    "    # Validation\n",
    "    evaluate(model, features, X_train, X_valid, y_train, y_valid)\n",
    "    \n",
    "    # Make predictions on test and generate submission dataframe\n",
    "    preds = model.predict(test[features])\n",
    "    sub = pd.DataFrame({'key': test_id, 'fare_amount': preds})\n",
    "    \n",
    "    # Extract feature importances\n",
    "    feature_importances = pd.DataFrame({'feature': features,\n",
    "                                        'importance': model.feature_importances_}).\\\n",
    "                           sort_values('importance', ascending = False)\n",
    "    \n",
    "    return sub, feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_dateinfo(df, date_col, drop=True, time=False, \n",
    "                     start_ref = pd.datetime(1900, 1, 1),\n",
    "                     extra_attr = False):\n",
    "    \"\"\"\n",
    "    Extract Date Information from a DataFrame\n",
    "    Adapted from: https://github.com/fastai/fastai/blob/master/fastai/structured.py\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract the field\n",
    "    fld = df[date_col]\n",
    "    \n",
    "    # Check the time\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    # Convert to datetime if not already\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[date_col] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    \n",
    "\n",
    "    # Prefix for new columns\n",
    "    pre = re.sub('[Dd]ate', '', date_col)\n",
    "    pre = re.sub('[Tt]ime', '', pre)\n",
    "    \n",
    "    # Basic attributes\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Days_in_month', 'is_leap_year']\n",
    "    \n",
    "    # Additional attributes\n",
    "    if extra_attr:\n",
    "        attr = attr + ['Is_month_end', 'Is_month_start', 'Is_quarter_end', \n",
    "                       'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    \n",
    "    # If time is specified, extract time information\n",
    "    if time: \n",
    "        attr = attr + ['Hour', 'Minute', 'Second']\n",
    "        \n",
    "    # Iterate through each attribute\n",
    "    for n in attr: \n",
    "        df[pre + n] = getattr(fld.dt, n.lower())\n",
    "        \n",
    "    # Calculate days in year\n",
    "    df[pre + 'Days_in_year'] = df[pre + 'is_leap_year'] + 365\n",
    "        \n",
    "    if time:\n",
    "        # Add fractional time of day (0 - 1) units of day\n",
    "        df[pre + 'frac_day'] = ((df[pre + 'Hour']) + (df[pre + 'Minute'] / 60) + (df[pre + 'Second'] / 60 / 60)) / 24\n",
    "        \n",
    "        # Add fractional time of week (0 - 1) units of week\n",
    "        df[pre + 'frac_week'] = (df[pre + 'Dayofweek'] + df[pre + 'frac_day']) / 7\n",
    "    \n",
    "        # Add fractional time of month (0 - 1) units of month\n",
    "        df[pre + 'frac_month'] = (df[pre + 'Day'] + (df[pre + 'frac_day'])) / (df[pre + 'Days_in_month'] +  1)\n",
    "        \n",
    "        # Add fractional time of year (0 - 1) units of year\n",
    "        df[pre + 'frac_year'] = (df[pre + 'Dayofyear'] + df[pre + 'frac_day']) / (df[pre + 'Days_in_year'] + 1)\n",
    "        \n",
    "    # Add seconds since start of reference\n",
    "    df[pre + 'Elapsed'] = (fld - start_ref).dt.total_seconds()\n",
    "    \n",
    "    if drop: \n",
    "        df = df.drop(date_col, axis=1)\n",
    "        \n",
    "    return df\n",
    "\n",
    "test = extract_dateinfo(test, 'pickup_datetime', drop = False, \n",
    "                         time = True, start_ref = data['pickup_datetime'].min())\n",
    "\n",
    "data = extract_dateinfo(data, 'pickup_datetime', drop = False, \n",
    "                         time = True, start_ref = data['pickup_datetime'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rounded Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_names = []\n",
    "# Round to two places\n",
    "for l in ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']:\n",
    "    \n",
    "    r = f'{l}-round'\n",
    "    data[r] = data[l].round(2)\n",
    "    test[r] = test[l].round(2)\n",
    "    rounded_names.append(r)\n",
    "    \n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_round = pd.read_csv('input/test_rounded_distances.csv')\n",
    "test_round.columns = rounded_names + list(test_round.columns[4:])\n",
    "test_round.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.merge(test_round, on = rounded_names, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_round = pd.read_csv('input/rounded_distances.csv')\n",
    "data_round.columns = rounded_names + list(data_round.columns[4:])\n",
    "data = data.merge(data_round, on = rounded_names, how = 'left')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "\n",
    "for i, (f, grouped) in enumerate(data.groupby('fare-bin')):\n",
    "    sns.kdeplot(grouped['distance'], label = f'{f}', color = palette[i])\n",
    "\n",
    "plt.xlabel('Distance'); plt.ylabel('Density'); plt.title('Distance Distribution by Fare Bin');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "\n",
    "for i, (f, grouped) in enumerate(data.groupby('fare-bin')):\n",
    "    sns.kdeplot(grouped['duration'], label = f'{f}', color = palette[i])\n",
    "\n",
    "plt.xlabel('Duration'); plt.ylabel('Density'); plt.title('Duration Distribution by Fare Bin');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data, np.array(data['fare_amount']), \n",
    "                                                      stratify = data['fare-bin'],\n",
    "                                                      random_state = RSEED, test_size = 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(data.columns)\n",
    "\n",
    "for f in ['pickup_datetime', 'fare_amount', 'fare-bin', 'pickup', 'dropoff'] + rounded_names:\n",
    "    features.remove(f)\n",
    "    \n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub, fi = model_rf(X_train, X_valid, y_train, y_valid, test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
